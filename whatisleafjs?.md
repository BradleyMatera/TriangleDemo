Leaf.js Technical Overview and Integration Guide

Overview and Vision

Leaf.js is a next-generation 2D/3D rendering engine for the web, aiming to bring near-native game graphics performance to browsers. It leverages modern browser graphics APIs – primarily WebGPU – to create high-fidelity real-time 3D experiences in web pages ￼. The design philosophy is to combine power and simplicity: developers describe what to render at a high level, and Leaf handles the low-level GPU work behind the scenes ￼. Ultimately, Leaf’s vision is to make web-based games as capable as desktop/console games, with support for advanced graphics techniques and efficient asset pipelines for industries like simulation, architecture, and VFX ￼ ￼.

Experimental status: WebGPU is still cutting-edge and may require enabling browser flags (e.g. chrome://flags/#enable-unsafe-webgpu in Chrome) ￼. Leaf.js is under active development (current version ~0.2.x) and is not feature-complete – many ambitious features (advanced lighting, physics, etc.) are still in progress ￼. Nonetheless, the library already lays an architectural foundation for high-performance rendering and provides core components to start building interactive scenes.

Architecture at a Glance

Leaf’s architecture is modular and emphasizes a unified scene graph approach. Both 2D and 3D content are handled within a single runtime hierarchy, so you can manage 2D overlays or UI and 3D objects together if needed ￼. The engine will automatically choose the best graphics backend available: it will try WebGPU first, and fall back to WebGL2 or even 2D Canvas if necessary, ensuring broad compatibility ￼.

Key architectural features include:
	•	Declarative Scene Definition: In the future, you will be able to define scenes using plain objects or even external files (e.g. YAML), describing models, lights, animations, etc., and Leaf will construct the scene graph for you ￼. This acts somewhat like a “virtual DOM” for graphics – you declare what should be in the scene, and the engine will diff/apply these to the actual GPU scene. (Currently this is partially implemented; you may still need to imperatively load assets and create objects, as the high-level API is evolving.)
	•	Reactive Update Loop: Leaf uses an event-driven model for frame updates. Instead of you manually managing requestAnimationFrame, you can attach callbacks to the scene’s lifecycle events (e.g. onStart, onUpdate). When a scene runs, it triggers these events each frame so you can update object properties or game logic in a reactive fashion ￼. For example, you can register an onUpdate handler to adjust a model’s position or rotate the camera every frame without writing your own loop – the engine calls it automatically during the render cycle ￼.
	•	Performance Optimizations: Though still in development, the design includes concepts like batched rendering, frustum culling, Level of Detail (LOD) management, and persistent GPU buffers to achieve high frame rates (targeting 1080p @ 120 FPS) ￼. Memory usage and multi-threading (e.g. for asset loading) are considered in the architecture. For instance, heavy file parsing (OBJ, FBX, PSD, etc.) can be offloaded to Web Workers to avoid blocking the main thread, using a work-queue system (discussed later).
	•	Extensibility: A plugin system is planned to allow custom rendering pipelines, shadow/lighting modules, physics engines, and other extensions to plug into the engine ￼. The core is kept modular so developers can replace or extend components (for example, swap out the default shader with a custom one, or integrate a different physics solver) without forking the entire engine. Leaf’s open design and TypeScript codebase make it feasible to tinker with internals or contribute new features ￼.

In summary, Leaf.js’s architecture bridges low-level GPU control with high-level developer ergonomics. You get direct access to performance-critical features (like GPU buffers, compute shaders, etc.) when needed, but the common boilerplate (context setup, render loop, shader compilation) is handled for you ￼. Next, we’ll break down the project structure and core components to understand how the library operates under the hood.

Project Structure and Core Components

The Leaf.js repository is organized into several TypeScript modules, each handling a part of the engine’s functionality. Below is a breakdown of the major files and their roles:
	•	src/index.ts: Entry point that re-exports the library’s public API. It aggregates key classes and utilities so developers can import them easily. For example, it exports the Scene, Renderer, Profiler, constants, built-in meshes, particle systems, etc. ￼.
	•	src/scene.ts – Scene Class: Defines the Scene class, which represents a collection of objects (models) rendered in one context (one canvas) ￼. The Scene is responsible for managing the lifecycle: it maintains a list of child models, holds a reference to a Renderer, and an event dispatcher for lifecycle events. When you create a Scene, you provide a target canvas element. You can then register callbacks via scene.awake(), scene.start(), and scene.update() for the respective events (scene initialization, on first frame, and on every frame) ￼. Calling scene.run() will fire the onAwake/onStart events and then enter the render loop (using requestAnimationFrame) that continuously calls the scene’s render method ￼. The Scene’s render() in the current code is a stub (just logging “Render”) – in a complete implementation it would delegate to the Renderer to draw the frame and also trigger any onUpdate callbacks each iteration. Scenes can be paused/resumed or fully stopped via pause(), resume(), and stop() methods ￼, which control the internal RAF loop. The Scene class also includes a simple mechanism to mark contexts as “static” (unmoving) vs dynamic, though as of now checkStatic() always returns true (this is likely a placeholder for optimizations) ￼. Internally, Scene keeps a map of children (the models in the scene) and can hold arbitrary user data (uData) for the scene ￼.
	•	src/renderer.ts – Renderer3D Class: Implements the Renderer3D class, which encapsulates the low-level rendering logic and GPU context management for 3D scenes ￼. When you instantiate a Renderer3D, you provide the target canvas; the renderer will attempt to initialize a WebGPU context on that canvas. Specifically, it will call an internal initWebGPU() (which uses navigator.gpu.requestAdapter() and adapter.requestDevice() to get a GPUDevice and configure a swap chain context) and prepare render targets (color texture, depth buffer, etc). If WebGPU isn’t available or fails, it has a fallback initWebGL() that tries to get a WebGL2 rendering context from the canvas ￼. The renderer keeps track of which context type was chosen in contextType ('webgpu' or 'webgl2') and stores the context object accordingly (either a GPUCanvasContext or a WebGLRenderingContext) ￼. Once the GPU context is ready, Renderer3D.init() can load a scene or model file by file name. Currently, static file-based scene definitions are supported – you pass a file path to init(), and the renderer will detect the format and load it ￼. Under the hood, it uses helper methods: loadFBXScene, loadOBJScene, loadSTLScene, etc., depending on the file extension, to parse the model data and create GPU resources. The Renderer3D class maintains internally a list of models (likely the parsed models/meshes) and a reference to a Camera. It also sets up some default GPU state: for example, it uses default vertex buffer layouts, primitive state, and depth/stencil state defined in the constants (discussed below) ￼. The actual drawing is done in a private render() method which would encode GPU draw commands for each model and submit them each frame (this method is invoked by the Scene’s loop or by requestAnimationFrame). We also see fields for picking and gizmos: the Renderer holds a pickTexture and pickTextureView (an off-screen texture for color-coded object picking, used to determine which object is under the mouse) and a gizmo for handling interactive transformations ￼. The setCamera() method allows attaching a Camera object to the renderer, so that its view/projection matrices can be used when drawing ￼. Overall, Renderer3D is the bridge between high-level scene/model data and the GPU: it prepares GPU buffers, compiles shaders, sets up pipelines, and issues draw calls each frame.
	•	src/camera.ts – Camera Class: Defines a camera that can be used to view the 3D scene ￼. The Camera class supports both perspective and orthographic projections (type can be 'perspective' or 'orthographic') ￼. Key properties include FOV (field of view in degrees for perspective cameras), near and far clipping planes, aspect ratio, and an optional zoom (useful for orthographic scaling) ￼. The camera keeps track of its transform: position, target, and up direction – these are stored as 4x4 matrices or vectors (Leaf uses the wgpu-matrix library for linear algebra, so Mat4 is used to represent points and directions) ￼. On initialization, the camera can compute a default projection matrix (either by createPerspectiveProjection() or createOrthographicProjection() as appropriate) ￼, and a view matrix via lookAt() which sets the camera’s position and orientation toward a target point ￼. The camera class also provides an updateProjectionMatrix(modelMatrix) function that will compute a combined Model-View-Projection matrix given a model’s transform, essentially MVP = P * V * M ￼. In use, you create a Camera and then attach it to the renderer; the renderer will use the camera’s matrices when drawing the scene so that objects are projected correctly.
	•	src/eventdispatcher.ts – EventDispatcher: Implements a simple publish/subscribe event system used by Leaf’s components ￼. It allows any part of the engine (especially Scene) to define custom events and listeners. The EventDispatcher keeps a map of event names to arrays of callback functions ￼. It provides on(event, callback) to register a listener, emit(event) to call all listeners for an event, and methods to remove listeners (off) or clear events ￼. This is used, for example, in Scene: when you call scene.update(fn), it does eventDispatcher.on('onUpdate', fn) to add your callback to the update event ￼. Later, each frame, the scene will emit('onUpdate') to invoke all such callbacks. The EventDispatcher is also envisioned to integrate with the Profiler (its comment notes that it allows rendering the event stack in the profiler for the leaf canvas) ￼, meaning the Profiler can inspect what events fired each frame for debugging. This event system is crucial for Leaf’s reactive features – it decouples the engine’s core loop from user logic, letting you inject behavior without modifying the engine loop itself.
	•	src/profiler.ts – Profiler Element: Leaf includes a built-in Profiler as a debugging aid. The Profiler is implemented as a custom HTML element that extends <canvas> ￼. It is designed to be an overlay canvas that displays performance metrics of a target Leaf canvas (frame rate, frame time graph, memory usage, and a call stack of events). In the code, Profiler (class name likely Profiler or LProfiler) is an extension of HTMLCanvasElement with special behavior: it defines static observedAttributes (like targetId) ￼ so that if you place <canvas is="leaf-profiler" targetId="myCanvas"> in HTML, the element knows which canvas to monitor. When connected, the Profiler’s connectedCallback() will find the target canvas by ID and start tracking it ￼. Internally, the Profiler hooks into the target’s rendering cycle (or uses requestAnimationFrame) to collect timing data. It keeps arrays of recent frame times to compute FPS ￼, tracks memory usage (if available via performance APIs), and even logs event call stacks (through callStackEvents) to show what events or updates occurred each frame ￼. The Profiler draws a heads-up display on itself: renderFrameTimeGraph draws the frame time/FPS graph, renderMemoryUsageGraph a memory bar chart, and so on ￼. You can toggle between a “performance” view and a “call stack” view (the Profiler has tabs and a method switchTab() to change view) ￼. It also has a mode to highlight bounding boxes (if showBoundingBoxes is true and you call BB() method, it would draw the bounding boxes of objects) ￼ ￼. To use the Profiler, you register the custom element (Leaf may do this for you when imported, or you might call customElements.define('leaf-profiler', Profiler, {extends:'canvas'}) if needed) and then add a canvas in your HTML with is="leaf-profiler" pointing to your main canvas. This will give you an in-browser performance panel for your scene, which is extremely useful during development.
	•	src/gizmo.ts – Gizmo Class: This defines an interactive transformation gizmo (the axes widget you often see in editors for moving/rotating objects) for Leaf. The Gizmo class creates a small set of GPU geometry (most likely arrow vectors or axis lines) and a pipeline to render them on top of the scene ￼. It holds a reference to the GPU device and creates its own render pipeline (with simple vertex and fragment shaders to draw colored axes). It stores a vertexBuffer (likely for the gizmo’s vertices), a uniformBuffer (probably to position/orient the gizmo in the scene or for color data), and a bindGroup to bind that uniform to the pipeline ￼. The Gizmo can attach mouse interaction handlers via attachInteraction(canvas, camera) ￼. This sets up event listeners on the given canvas for mouse down/move/up. When you drag the gizmo’s axis, it will project the mouse movement into 3D (using projectToScreen math) and move the selected axis accordingly – effectively allowing click-and-drag object transforms. It tracks which axis is active (activeAxis) and whether it’s being dragged (isDragging), as well as the last mouse position ￼. On init(), the gizmo creates its GPU resources (calls createShader() for its shader and sets up the pipeline with the given color texture format) ￼. On each frame, the Renderer can call gizmo.draw(passEncoder) to render the gizmo on top of the scene (likely after drawing all models). The gizmo is a developer tool to manipulate objects visually; in Leaf’s incomplete state it might not yet tie into a full editor, but the structure is there to support translation/rotation of objects via the GUI.
	•	src/constants.ts: Contains various engine-wide constants and defaults. For example, it defines DEFAULT_VIEW_FRUSTUM (the default camera frustum size or field-of-view) and default GPU pipeline states for rendering ￼. It includes DEFAULT_PIPELINE_BUFFERS (an array of GPUVertexBufferLayout definitions specifying how vertex buffers are structured – likely position, normal, UV attributes in a mesh) ￼, DEFAULT_PRIMITIVE_STATE (a GPUPrimitiveState object, probably setting triangle list topology, backface culling, etc.), and DEFAULT_DEPTH_STENCIL (a GPUDepthStencilState enabling depth testing so 3D objects occlude properly) ￼. It also defines SUPPORTED_FILETYPES, a set of file extensions that the engine knows how to parse ￼. This likely includes extensions like "obj", "fbx", "stl", "aseprite", "psd", and scene definition formats like "yaml" or "json". The renderer uses this to decide which parser to invoke for a given file.
	•	Built-in Meshes (src/meshes/): The library provides some primitive 3D shapes out-of-the-box. In the meshes folder, you’ll find modules like cube.ts, capsule.ts, quad.ts etc. These define geometry data for common shapes so you can quickly add simple objects without needing external files. For example, cube.ts exports a constant cubeVertexArray (a Float32Array of the cube’s vertices), along with constants like cubeVertexCount = 36 (since a cube has 36 vertices for 12 triangles), offsets for color/UV data in the vertex, and so on ￼. These arrays contain interleaved vertex attributes (position, normal, UV, etc.) and their sizes are defined by cubeVertexSize, cubePositionOffset, cubeColorOffset, etc. ￼. You can use these to create a GPU buffer and draw a cube immediately. The engine might internally use these for debugging or default scenes.
	•	Shader Modules (src/shaders/): Leaf.js includes default shader code in WGSL (WebGPU’s shading language). In the shaders directory, there are files like basic.vert.wgsl and color.frag.wgsl (compiled to .d.ts strings in the package) which define a very simple rendering shader. The basic vertex shader transforms vertex positions by the combined model-view-projection matrix and passes through UV coordinates and the position for use in the fragment shader ￼. The color fragment shader simply outputs a color based on the fragment position or some other simplistic scheme (in the code, it returns fragPosition as the output color) ￼. This essentially gives a rudimentary unlit coloring (e.g., encoding the 3D position into RGB for a colorful effect). These shaders are likely used as placeholders – for example, the OBJ/FBX parsers set a shaderString property with WGSL code. The comment in the FBX parser mentions a “basic Lambert/Phong shader with proper alignment padding” ￼, so it may generate or use a WGSL shader that implements a simple lighting model. The presence of these shader modules suggests Leaf can compile its own shader programs; advanced users could swap in custom shader code by modifying the parser’s shader string or by creating their own pipeline.
	•	Asset Parsers (src/parsers/): One of Leaf’s strengths is supposed to be dynamic asset parsing – it supports loading various 3D and 2D file formats into the engine at runtime ￼. The parsers directory contains modules for each supported format:
	•	OBJ/MTL Parser: The OBJ format parser is implemented in obj.ts and likely a related material parser in mtl.ts. The OBJ parser (WebGPUOBJParser class) reads an OBJ model file (text format) and extracts vertices, normals, texture coordinates, and indices ￼. It stores these in arrays (vertices: Vertex[] and indices: number[]) and can create GPU buffers from them. The parser class has methods like loadOBJ(objData: string): Promise<void> to parse the text, createBuffers() to upload data to the GPU, and createPipeline(shaderModule, format, sceneUBO, materialUBO) to create a render pipeline for the object ￼. It also can provide a default GPUShaderModule via getShader() (compiling the internal shaderString) and getters for the GPU buffers ￼. The parser prepares a bindGroup tying together the object’s vertex buffer, index buffer, and material uniform buffer with the pipeline ￼. The MTL parser (parseMtl function in mtl.ts) handles Wavefront material files. It reads the properties for each material (Ka, Kd, Ks colors, Ns shininess, texture map filenames, etc.) and returns a map of material name to a structured MtlMaterial object ￼ ￼. It also provides utility to create a GPU uniform buffer from a material: createMaterialUniformBufferData(material) which packs an MtlMaterial’s fields into a 96-byte Float32Array matching the shader’s expected uniform structure ￼. This suggests the default shader uses a uniform with 24 floats for material properties (e.g., ambient, diffuse, specular colors etc.). If an OBJ model comes with an MTL, Leaf can use this to render basic material color; texture map filenames are noted but actual texture loading is likely not fully implemented in the current version (this might be a gap to fill).
	•	FBX Parser: FBX is a complex binary format for 3D scenes (often with animations). The fbx.ts defines parseFBX(data) and a WebGPUFBXParser class ￼. The FBX parser likely uses a binary reader (it might utilize the LeafView class, described below) to extract mesh geometry. In the FBX interface, FBXMesh contains arrays for vertices, indices, normals, and UVs ￼. The WebGPUFBXParser works similarly to the OBJ parser: it has loadFBX(source) to read the file (string or ArrayBuffer), stores vertices and indices (as typed arrays), and creates GPU buffers for them ￼. It, too, has a shaderString (likely a default shader for FBX meshes) and methods to compile shader and pipeline, and a render(passEncoder) to issue draw calls for the mesh ￼. The comment indicates it uses a “basic Lambert/Phong shader with proper alignment padding,” so presumably it shades FBX models with simple lighting using normals. FBX files can contain multiple meshes and possibly an entire scene graph – it’s not clear if this parser handles only one mesh or multiple. Given the engine state, it might just load a single mesh (e.g., the first mesh or a combined mesh) for now. Animation (skeletons, etc.) from FBX is likely not yet supported, as that would require a lot more infrastructure (skeleton classes, animation loops).
	•	STL Parser: STL is a simpler format (triangles soup, no materials). While we didn’t open stl.ts, we expect it reads an STL (ASCII or binary) and creates a mesh of vertices and normals. It probably follows a pattern similar to OBJ.
	•	Aseprite Parser: In parsers/aseprite/, there is an Aseprite parser for .ase or .aseprite files (these are 2D sprite animations with frames and layers, often used for pixel art). This is a very advanced feature for a web engine, showing Leaf’s goal of unifying 2D/3D content. The Aseprite parser code is quite extensive: it defines data types for frames, layers, cels, palettes, etc. in types.d.ts (nearly 5 KB of type definitions) ￼. The Aseprite.ts class can take an ArrayBuffer of an .ase file and parse out the sprite frames. The AseView class extends the base LeafView (which is a binary DataView helper) to read specific chunks of the Aseprite file format ￼. It has methods to read each chunk type: e.g. layer(), cel(), palette(), tags(), etc., corresponding to sections of the Aseprite format ￼. It even handles compressed data (there’s an inflate() method to decompress zlib-compressed image data within the file) ￼. Using these, the parser builds an Aseprite object that contains all frames, the canvas size, and animation tags ￼. Leaf provides a convenience function loadAseprite(filePath, options) to fetch and parse an Aseprite file, returning a Promise that resolves to an Aseprite object ￼. In practice, once you have an Aseprite object, you could use it to create a sprite animation in a 2D scene or as a texture on a 3D surface. The Aseprite parser demonstrates Leaf’s heavy use of Web Workers for parsing – likely you’d want to parse large binary files off the main thread. (Leaf’s WorkQueue, discussed later, can be used here.)
	•	PSD Parser: Similarly, there is a psd/ directory hinting at a Photoshop PSD file parser. If implemented, it would parse layers and images from PSD for use as textures or sprites. The details aren’t in the snippet, but the intention is to support loading layered artwork directly.
	•	LeafView (Binary Reader): The LeafView class in parsers/LeafView.ts is a utility that extends JavaScript’s DataView to simplify reading binary files ￼. It provides methods like byte(), word(), dword(), etc., to read values of various sizes, as well as higher-level methods to read strings (str()), arrays of bytes, and even specific structured patterns (pairs, triplets, etc.) ￼. This is heavily used by the FBX and Aseprite parsers to navigate their binary formats. By using LeafView, the parsers can do things like view.string(length) to read a fixed-length string, or view.float() to read a 32-bit float, advancing the internal offset each time. This utility makes writing new binary format parsers easier.
	•	Particle System (src/particles/): Leaf.js includes an experimental particle system that uses GPU compute for efficiency. The ParticleRenderer class encapsulates this system ￼. When you create a ParticleRenderer, you provide a canvas and an optional config object defining the particle parameters (count, emission rate, area, initial velocity, lifespan, gravity, etc.) ￼ ￼. The ParticleRenderer then initializes its pipelines: it creates a compute shader to update particle positions each frame (accounting for gravity, lifetime, etc.) and a render shader to draw the particles (likely as points or quads). Internally it allocates a GPU buffer to hold all particle data (position, velocity, life, etc.), a uniform buffer (for global parameters like time), and sets up two pipelines: one for compute and one for rendering ￼. For example, particleShaders.ts defines shader code strings for two modes – “simulation” and “standard” – each providing a render and compute shader variant ￼. These might correspond to whether particles are continuously emitted (simulation) or one-shot. The config (ParticleConfig) lets you toggle whether to run the simulation or not (simulation?: boolean), how many particles, gravity strength, etc. ￼. The ParticleRenderer’s private methods include init() and initBuffers() to set up GPU state, createPipelines() to compile the compute and render pipelines, emitParticles() to spawn new particles, and an internal update() that the compute shader will run every frame to advance particle states ￼. It also has startRenderLoop() which likely kicks off a separate render loop for the particles (or integrates with the main scene loop). In usage, you can simply instantiate a ParticleRenderer with a canvas and config, and it will start animating particles on that canvas. This is great for effects like smoke, fire, rain, etc., offloading the heavy lifting to the GPU. You could also overlay this particle canvas on a 3D scene (e.g., via CSS layering) if you need particle effects in your game. Since the particle system uses compute shaders (WebGPU feature), it will only run with WebGPU (no WebGL fallback for this part).
	•	2D Rendering Module (src/2d/): Leaf.js isn’t just about 3D – it also provides utilities for simpler 2D rendering (think of it like a lightweight 2D engine using the HTML5 Canvas 2D context). The 2d folder contains:
	•	Renderer2D: A class to simplify drawing to a 2D canvas on a schedule. It wraps a standard CanvasRenderingContext2D and provides a game-loop mechanism ￼. When you create a Renderer2d(canvas, drawFunction, options), you pass it a draw callback. This drawFunction(renderer, delta) is a function that will be called every frame with the renderer and the elapsed time since last frame. The Renderer2d stores the context (context: CanvasRenderingContext2D) and a target frame rate (fps) ￼. Its start() method begins a loop (internally it might use requestAnimationFrame or setInterval depending on fps) to repeatedly clear the canvas and call your draw function. The comment notes that in 2D mode, it basically clears the rect and redraws at the desired FPS ￼. There is also a stop() to halt the loop ￼. This is handy for quickly making a canvas-based animation or GUI. Rather than managing your own RAF, you can let Renderer2d handle timing and just focus on drawing.
	•	Scene2D: A minimal scene-like class for 2D. It mainly holds a reference to the canvas and perhaps could manage a set of sprites or layers, but in the current code it’s very simple ￼. The Scene2d.render(renderer, delta) method calls the provided Renderer2d’s draw function with the given delta time ￼. In practice, you might not need to use Scene2d at all – you can just use Renderer2d directly. Scene2d seems to exist for API consistency (so one could imagine a unified interface between 3D scenes and 2D scenes), but it currently doesn’t do much beyond organizing data to pass to the renderer.
	•	SpriteManager: This class is marked experimental. It is intended to handle loading and managing sprite images or sprite sheets, potentially using Web Workers for processing ￼. The SpriteManager likely works with the asset parsers (Aseprite, PSD). It has methods like loadSprite() (for a simple image or sprite sheet), loadAseprite(), and loadPsd() ￼. The idea is that you could give it an image or Aseprite file, and it would produce an easy-to-use sprite animation or frames that you can draw on the canvas. Since it mentions utilizing a WebWorker, it probably offloads decoding or big parsing tasks to a separate thread via the WorkQueue.
	•	WorkQueue: This utility class abstracts message-passing to a Web Worker, supporting a queue of tasks. You initialize it with a worker script URL, and then call send(message) which returns a Promise that resolves when the worker responds ￼. WorkQueue ensures that if multiple tasks are sent, they are queued and handled in order, and that you can await their results easily. Leaf might spawn a worker that runs heavy parsers (e.g., parsing FBX or generating sprite frames) so the main UI stays responsive. The WorkQueue helps coordinate this without cluttering application code with postMessage handlers.
	•	Utilities (src/utils/): Contains general helper functions. For example, util.ts defines some assertion functions like assert(condition, msg) which throws an error if a condition is false (used to catch misuse or invalid states in dev mode) ￼, assertOK(value) to throw if a value is an Error, and unreachable() to mark code paths that should never execute ￼. These help with debugging and enforcing correct API usage (for instance, asserting that WebGPU is supported before proceeding, etc.). There might be other math or data structure utilities (the presence of things like Pair, Triplet, Quad types for generic tuples ￼ suggests some utility for dealing with coordinate pairs/triplets).

This structure shows that Leaf.js is quite comprehensive: it not only sets up a WebGPU rendering loop, but also parses multiple asset types, provides debugging tools (Profiler, Gizmo), and even has a foot in 2D canvas rendering. Next, we’ll discuss how the rendering pipeline works at runtime – i.e., how all these components come together when you actually run a scene.

Rendering Pipeline and Runtime Flow

When you run a Leaf.js scene, the engine orchestrates a sequence of steps each frame to draw your content. Let’s walk through the typical flow, from initialization to each render loop iteration:
	1.	Canvas and Context Initialization: You begin by creating a <canvas> element in your HTML (or using the provided <leaf-canvas> custom element if available – Leaf intends to offer a custom canvas element, though currently you may just use a normal canvas). This canvas is where the engine will render the scene. When you create a Renderer3D for this canvas, it attempts to acquire a GPU context. If WebGPU is available, it calls navigator.gpu.requestAdapter() and adapter.requestDevice() to get a GPUDevice. With that, it sets up a GPUCanvasContext for the canvas (by calling canvas.getContext('webgpu')) and configures the context’s format (typically a color texture format like "bgra8unorm" for the swap chain). It also creates a depth buffer texture for depth testing. If WebGPU is not available (e.g., in an unsupported browser), the Renderer3D will fall back to WebGL2: it calls canvas.getContext('webgl2') (or 'webgl' as last resort) to get a GL context ￼. In this mode, Leaf would use WebGL calls to draw; however, note that much of Leaf’s code (shaders, pipeline objects) is WebGPU-specific, so the WebGL path may be limited or still under development. The contextType field in Renderer3D is set to indicate which path was taken ('webgpu' or 'webgl2') ￼. This initialization happens when you call renderer.init(). If using the high-level Scene API, calling scene.run() will internally ensure the renderer is ready as well (for example, you might call renderer.init() before scene.run()).
	2.	Loading Assets (Models/Scenes): You then load content into the scene. There are two approaches:
	•	File-based scene: You can call renderer.init('path/to/file') to load a model or scene file. The engine looks at the file extension and dispatches to the appropriate parser. For example, if you pass 'scene.fbx', it will call loadFBXScene which uses the WebGPUFBXParser to parse the FBX file and create mesh data ￼. If you pass 'model.obj', it calls loadOBJScene using the OBJ parser, and similarly for 'something.stl'. For OBJ, after parsing, it will likely also load the referenced .mtl (if present) to get material info. These parsers produce one or more Model objects (which contain the GPU buffers, pipeline, etc.). The Renderer keeps track of these, perhaps in an internal list (this.models). If the file was a full scene definition (e.g., a YAML describing multiple models), the engine would create multiple model entries. At this stage, the engine compiles required shaders and pipelines: Each parser either provides a WGSL shader or uses the default ones. The engine knows the layout of vertex data (position, normal, UV – see DEFAULT_PIPELINE_BUFFERS) ￼ and it knows the format of the uniform buffers (for camera and material). It creates a GPURenderPipeline with that layout, attaching the compiled vertex and fragment shaders. For instance, the FBX parser after load would call createPipeline(shaderModule, format, sceneUBO, materialUBO) which sets up a pipeline for drawing that mesh with the given render target format and uniform buffers bound ￼. This pipeline creation uses default rasterization state from constants (triangle list, backface culling on, depth test on from DEFAULT_PRIMITIVE_STATE and DEFAULT_DEPTH_STENCIL) ￼. If using WebGL fallback, there isn’t a formal pipeline object; instead, the engine would set up shader programs and vertex attribute pointers analogous to those pipeline settings.
	•	Programmatic scene: Alternatively, (in future or with partial support now) you could create a scene by instantiating models directly. For example, you might use the built-in meshes: construct a GPU buffer from meshes/cubeVertexArray and assign a basic shader to it. Leaf doesn’t yet have a polished API for this, but conceptually you could manually use the Model interface to add objects. The Model type (from scene.types.ts) defines fields like vertexBuffer (GPUBuffer for vertices), indexBuffer, an associated shader name or code, a unique id and name, a static flag, and a pickingColor for object picking ￼. In the future, Leaf.scene({...models: {...}}) would create these models for you. Currently, you might directly manipulate the Renderer’s models list or call low-level methods to add geometry. The engine is not fully reactive yet (i.e., you cannot just push a new model into the scene and have it render next frame unless you also update the GPU pipeline accordingly). So file loading is the main path used in this version.
	3.	Setting Up the Camera: For 3D content, you should create a Camera and position it. The camera’s projection matrix needs to match your scene (a good FOV for a 3D scene is ~60°, near plane maybe 0.1, far plane maybe 1000). If you don’t explicitly set a camera, the Renderer might default to some basic camera (possibly using DEFAULT_VIEW_FRUSTUM for FOV) ￼. But typically you do:

const camera = new Camera(60, /*viewportWidth*/ canvas.width, 0.1, 1000);
camera.lookAt([0, 0, 5], [0, 0, 0]);  // position at (0,0,5) looking towards origin
renderer.setCamera(camera);

This ensures the renderer has the view and projection matrices ready. On each frame, the engine will update a scene uniform buffer with the camera’s matrix (MVP). For WebGPU, this is likely a small GPU buffer bound at group(0) binding(0) for the vertex shader (as seen in the basic.vert WGSL: it expects a uniform with modelViewProjectionMatrix ￼). Leaf probably maintains one global uniform buffer for the camera transform, updating it whenever the camera or model moves. If multiple models use different transforms, either the engine will update that uniform for each draw or use separate model transform uniforms (not fully clear, but since the vertex shader uses MVP, they may multiply model and view-projection on the CPU and send one matrix).

	4.	Running the Render Loop: With assets loaded and camera set, you start the scene. If using the Scene class, you call scene.run(). This will emit the 'onAwake' event (one-time initialization callbacks) and 'onStart' (on first frame) ￼, then enter a continuous loop using requestAnimationFrame ￼. Each iteration of the loop, the Scene calls its render() method. In a completed engine, Scene.render() would likely: update any internal animations, trigger 'onUpdate' events for user logic, and then delegate to renderer.render(). In the current snippet, Scene.render() just logs a message ￼ – clearly a placeholder – so it’s up to us to tie the pieces. In practice, Leaf’s Renderer3D might start its own RAF when you call renderer.init(), or you might manually call a render function. The intended design is probably: scene.run() calls renderer.render() inside the loop (possibly after doing event dispatch). So let’s assume the loop calls renderer.renderFrame() each tick.
In each frame of the render loop, the following happens:
	•	If a Profiler is attached, it might record the time at the start of the frame.
	•	The Scene (or your code) triggers any logic updates via event callbacks (e.g., you moved an object in an onUpdate callback, or AI/physics updated some state).
	•	The GPU draw commands are prepared: For WebGPU, the renderer acquires the next swap chain texture from the GPUCanvasContext, then begins a command encoder and render pass. It sets the pipeline state for each model and issues a draw. For example, for each model in renderer.models:
	•	It calls passEncoder.setPipeline(model.pipeline).
	•	It binds the vertex buffer(s) and index buffer (passEncoder.setVertexBuffer(0, model.vertexBuffer) etc.).
	•	It binds the uniform buffers for camera and material via passEncoder.setBindGroup(0, sceneBindGroup) and perhaps passEncoder.setBindGroup(1, materialBindGroup). These bind groups were likely created during init, containing the camera’s projection matrix and the material properties for that model.
	•	Finally, it calls passEncoder.drawIndexed (or draw if no indices) with the model’s index count to render the triangles.
	•	If the gizmo is enabled (e.g., an object is selected), after drawing all models, the renderer calls gizmo.draw(passEncoder) to overlay the transform gizmo. The gizmo’s pipeline likely draws lines or arrows in screen space or object space accordingly.
	•	The render pass is then ended and the command buffer submitted to the GPU queue.
	•	For WebGL fallback: it would bind the WebGL program and set up vertex attribute pointers similarly, then issue gl.drawElements for each object.
	•	If picking is implemented: The renderer might do a second pass (or an off-screen pass) where it draws each model with a flat unique color (using an off-screen pickTexture). This wasn’t explicitly shown, but since pickTexture exists, the likely scenario is: each model has a unique pickingColor (an RGB stored in Model.pickingColor ￼). The engine could render the scene to a small offscreen texture with each fragment outputting its object’s pickingColor instead of the normal shader. Then, on mouse click, it reads the pixel from that texture under the cursor to determine which model was clicked. This is how the Gizmo knows what you clicked on, or how a user selection would work. Implementation details aside, just be aware that such a mechanism is intended.
	•	The Profiler (if present) gathers metrics: it pushes the current frame time into the frameTimes array, calculates FPS (for example using a rolling window of frame times), and maybe records JS memory usage via performance.memory if available ￼. If the Profiler’s call stack tracking is enabled, the engine might log events or function calls (for example, every time an eventDispatcher.emit happens, it could call Profiler.addEventToCallStack(eventName, args) to record it ￼). The Profiler’s canvas will then redraw its graphs. Notably, the Profiler is itself using a 2D rendering (CanvasRenderingContext2D). It likely calls its render() method each frame which draws the FPS graph, etc., on the Profiler canvas overlay ￼. This happens concurrently with the 3D scene rendering (since they are just two canvases being updated).
	•	The frame is done. The Scene loop will then request the next animation frame, unless the scene was paused or stopped. If scene.pause() was called, a flag stops scheduling the next RAF. If resume() is called, it sets the flag and continues the loop.
Additionally, if the engine supports animations (not fully implemented yet), it would interpolate or advance animations tied to models each frame. The readme’s theoretical API shows an animations section where you could define, say, a rotation animation for an object over 1 second ￼. In a future version, the engine would decrement timers and update object transforms accordingly in the update loop.
	5.	2D Rendering Flow: If you are using the 2D Renderer (Renderer2d) separately or in conjunction with 3D, its loop is independent. When you call renderer2d.start(), it sets up its own requestAnimationFrame loop to call renderer2d.render(time) each tick ￼. The render(time) method in Renderer2d checks the elapsed time since last frame and if it meets the timing for the desired FPS, it calls the user-provided draw function ￼. In that draw callback, you use the standard Canvas2D API (e.g., draw shapes, images, text). Renderer2d ensures the canvas is cleared if needed and then yields control to your drawing code. If you specified an fps option, it may throttle the calls to achieve that frame rate instead of running as fast as possible. The 2D and 3D systems are separate – you could have one canvas running a 3D scene via Renderer3D and another canvas running a UI or mini-game via Renderer2d. They won’t block each other (aside from normal CPU scheduling), and you can even overlay them (e.g., a transparent 2D canvas on top of a 3D canvas for HUD).
	6.	Particle System Flow: The ParticleRenderer, once created, likely immediately starts its update loop (unless there is a method to start it explicitly). Internally, it uses the GPU’s compute capabilities. Each frame (perhaps on its own RAF or tied into a scene’s RAF), it will do:
	•	Update a time uniform (increasing a time or frame counter).
	•	Call the compute shader to update particle positions. This shader runs in GPU parallel, advancing each particle by its velocity, applying gravity, decreasing its life, etc. The gravity parameter from config is baked into the compute shader code (notice particleShaders.standardShaders.compute(gravity) produces a WGSL shader string with gravity constant ￼). The compute pass might also handle respawning particles whose life expired (if emission is continuous).
	•	Then it calls the render shader pipeline to draw all particles. Likely it draws them as points or quads using instancing. The render shader might simply output a constant color or a texture for each particle; details depend on how advanced it is. Given the minimalism of the provided shader strings, it might just output a color based on lifespan or so.
	•	These GPU operations are efficient, allowing potentially thousands of particles with minimal CPU cost. If the ParticleRenderer is not automatically tied to a Scene, you may need to overlay its canvas or manually call it per frame. However, since it has startRenderLoop() internally, it’s probable that constructing it kicks off an internal loop. For synchronization, if you wanted particles in the same scene as 3D objects (e.g., particles affected by camera perspective), you would likely have to integrate it manually by using custom shaders or feeding the camera matrix to the particle system. As of now, the particle system might be most useful for 2D effects or simple overlay effects.

In summary, the runtime rendering pipeline for Leaf.js involves setting up GPU state (via WGPU or WebGL), then on each frame updating any dynamic data (uniforms for camera, maybe vertex positions for moving objects or particles) and issuing draw calls for each object. The engine uses triple buffering style resource management (the WebGPU swap chain gives a new frame texture each time) and makes heavy use of GPU features: for instance, the entire particle simulation runs on the GPU via compute shaders, freeing the CPU to handle game logic. The design also allows for parallelism and background loading: the WorkQueue can parse models on a web worker thread, and once ready, hand them to the main thread to create GPU buffers. This way, a large model file won’t freeze your rendering; you could even show a loading spinner in the meantime.

Virtual DOM Analogy: While Leaf.js doesn’t literally manipulate the browser DOM for rendering (it draws to a canvas, not to HTML elements), it does maintain a virtual scene graph in memory. This is analogous to a virtual DOM: the scene graph (with Scene, Models, etc.) represents the desired state of the world, and the Renderer is responsible for reconciling that with GPU state. When the scene graph changes (e.g., you add/remove a model or change a property), ideally the engine would detect it and update the GPU resources accordingly. In the current state, this is somewhat manual – e.g., if you want to remove a model, you’d likely have to remove it from scene.children and also free its GPU buffers. But the framework is laid to eventually automate this, so that you could, for example, call something like scene.addModel({...}) and under the hood it creates the necessary buffers and pipelines. This is why the project emphasizes a declarative API in the docs ￼ – in the future you might just provide a JSON or object describing the scene, and Leaf will handle creating/updating/removing the actual GPU resources, much like how a virtual DOM diff would create/update/remove real DOM nodes.

Reactive Updates: As mentioned, Leaf’s main reactive mechanism is the event system (onUpdate, etc.). Instead of the engine trying to automatically detect property changes, it gives you hooks to change things at the right time. For example, if you want an object to rotate continuously, you don’t manually recalc its matrix in some loop external to Leaf; you attach an onUpdate callback to the scene:

scene.update(() => {
  model.rotation.y += 0.01;
});

Then each frame, that runs and you adjust the model’s rotation property, which will affect the next render (assuming the renderer uses the model’s transform when drawing). This pattern is reactive in the sense that your update logic is triggered by the system’s heartbeat (the frame loop) rather than by you having to poll or manage time. The engine could extend this with more sophisticated reactive features – e.g., imagine if model properties were proxies that automatically flagged the renderer to update a uniform buffer when changed. That’s not implemented yet, but could come. Currently, you might need to call a method to update GPU buffers if you change geometry (the engine doesn’t constantly stream dynamic geometry unless flagged). But for things like moving an object or changing its color, doing it in onUpdate and maybe calling a simple function to update a uniform is enough.

Using Leaf.js in Your Project

Even though Leaf.js is in early development, you can experiment with it and even integrate parts of it into your projects (keeping in mind some features are incomplete). Below are practical steps and examples to get started:

Installation and Setup
	•	Get the Library: You can install Leaf.js via npm: npm install leaf (the package name is leaf, not “leaf-js”). This will add it to your project. Alternatively, use a CDN – for example, include a script tag from jsDelivr: <script src="https://cdn.jsdelivr.net/npm/leaf@2.0.3/dist/bundle.js"></script> to get the UMD bundle (which exposes a global Leaf object). Leaf.js is Zlib licensed and currently version 2.0.3 on npm ￼.
	•	Enable WebGPU: Since WebGPU is still new, ensure it’s enabled in your browser. In Chrome Canary or newer versions of Chrome, go to chrome://flags and enable “Unsafe WebGPU”. In Firefox, set dom.webgpu.enabled to true in about:config. If you don’t do this, Leaf will likely fall back to WebGL2. This fallback allows things to run, but some advanced features (compute shaders for particles, etc.) will not work without WebGPU.
	•	Include the Canvas: In your HTML, create a canvas element for the 3D scene. For example:

<canvas id="gameCanvas" width="800" height="600"></canvas>

It’s often useful to style it with CSS to ensure it fills the container or screen as desired. If you plan to use the Profiler, also include a profiler canvas:

<canvas is="leaf-profiler" id="profiler" width="800" height="100" 
        targetId="gameCanvas" style="position:absolute; bottom:0; right:0;"></canvas>

The above profiler canvas (positioned absolute) will overlay a small performance graph at the bottom-right of your scene. (Note: if using the npm module via import, you might have to manually define the custom element for profiler as shown earlier).

	•	Import Leaf Classes: In your JS/TS code, import what you need from the leaf package. For a basic 3D scene, you’ll typically use Scene, Renderer3D, Camera, and maybe Profiler if you want to register it manually:

import { Scene, Renderer3D, Camera } from 'leaf';

If using the global UMD, skip imports and use the global (e.g., new Leaf.Scene() etc.).

Initializing a 3D Scene

Here’s a step-by-step example to load a 3D model (OBJ format) and display it:
	1.	Create Scene and Renderer:

const canvas = document.getElementById('gameCanvas');
const scene = new Scene(null, canvas);            // Initialize a Scene for this canvas
const renderer = new Renderer3D(canvas);
scene.renderer = renderer;                        // Associate the renderer with the scene

We pass null for the scene’s user data in this case. The scene now knows which canvas to use and has an event dispatcher ready. We also create a Renderer3D for that canvas and link it. At this point, the renderer has selected a GPU context (WebGPU or WebGL) as described earlier.

	2.	Set Up Camera:

const camera = new Camera(60, canvas.width / canvas.height, 0.1, 1000);
camera.lookAt([0, 2, 5], [0, 0, 0]);  // position the camera at (0,2,5), looking at origin
renderer.setCamera(camera);

This creates a perspective camera with 60° FOV, appropriate aspect ratio, near plane at 0.1, far plane at 1000. We then point it to look at the origin from a slight elevation. Now the renderer has the camera’s view/projection. You could skip this and use default, but then you might get a default top-down view (depending on default frustum). It’s better to explicitly set the camera.

	3.	Load a Model:

await renderer.init('models/teapot.obj');

This loads an OBJ file (ensure the path is correct and that your server serves it with correct CORS if needed). Under the hood, the OBJ parser will run and the teapot’s geometry will be uploaded to the GPU. If there’s a teapot.mtl, it will parse that too for material. Once this promise resolves, the renderer’s internal models list now has the teapot model ready to draw. If you had multiple files or a complex scene, you could call init() on a scene file (e.g., a .leaf.yml in future). Right now, load one model at a time – if you need more, you might call renderer.init() for each or use custom code to add multiple models.

	4.	(Optional) Register Update Logic: If you want to animate or interact, use scene’s event system. For example:

scene.update(() => {
  // Rotate the model a bit each frame
  const model = renderer.models[0];            // assuming one model
  if (model && !model.static) {
    // (Pseudo-code: you'd have to actually store model transformation in a matrix or angles.
    // For demonstration, suppose model has a rotation property or matrix we can modify)
    model.rotationY += 0.01;  // rotate around Y axis
  }
});

This will execute every frame before rendering. We check that the model exists and is not static (just in case). In a real scenario, you would integrate with a proper transformation system. Perhaps the Model interface would be extended to include position/rotation, or you maintain a separate transform matrix and update it here, then update the uniform buffer for that model’s transform. Since the current code doesn’t show a full transform system, we illustrate the concept. The key is that any changes you make here (like adjusting rotation) will reflect in the next draw call.

	5.	Run the Scene:

scene.run();

This starts the main loop. It will trigger the onStart event (you could have set up a scene.start(() => { console.log('Scene started'); }) if desired) and then begin drawing frames. You should now see your model on the canvas, rendered with whatever default shader is in place (likely a simple color or position-based color). If the model appears black or oddly colored, it’s because the default fragment shader might be outputting a value based on position or a constant. In future, this will be a proper lit shader.

	6.	Add Profiler (optional): If you included the profiler canvas in HTML, it should automatically start since it observes the target canvas. If not, you can create one via script:

import { Profiler } from 'leaf';
customElements.define('leaf-profiler', Profiler, { extends: 'canvas' });
const profilerCanvas = document.getElementById('profiler');
// The above canvas already has targetId="gameCanvas", so once defined, it links itself.

You’ll see the FPS graph and possibly the call stack tab if you click on it (depending on implementation). This helps you monitor performance.

Using this flow, you can load other formats too. For example, if you have an FBX:

await renderer.init('scene.fbx');

It might bring in a whole scene (if the FBX has multiple meshes, presumably the parser would load them all – though if not, it might just pick the first mesh). Similarly, renderer.init('model.stl') for STL files.

If you want to create a shape without an external file, you could use the built-in meshes. While an official API isn’t there yet, you can do something like:

// Create a GPU buffer for a cube using built-in data:
const cubeData = Leaf.cubeVertexArray;  // Float32Array of cube vertices
const device = renderer.device;         // GPUDevice obtained by Renderer3D
const cubeBuffer = device.createBuffer({
  size: cubeData.byteLength,
  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
});
device.queue.writeBuffer(cubeBuffer, 0, cubeData.buffer);

And then you’d have to somehow tell the renderer to use this buffer with a pipeline. Currently, you might have to simulate an OBJ load or manually create a Model object and push it to renderer.models. This is advanced usage – for now, sticking to renderer.init(file) is simpler.

Integrating 2D Rendering

Leaf’s 2D utilities can be used for HUDs, mini-games, or as a standalone. Suppose you want to draw a simple 2D overlay (like text or UI) on top of the 3D canvas. You can absolutely put an HTML canvas on top of the WebGPU canvas and use Renderer2d for it:

import { Renderer2d } from 'leaf';
const hudCanvas = document.getElementById('hudCanvas');
const ctx = hudCanvas.getContext('2d');
const hudRenderer = new Renderer2d(hudCanvas, (renderer, delta) => {
  const ctx = renderer.context;
  ctx.clearRect(0, 0, hudCanvas.width, hudCanvas.height);
  ctx.fillStyle = 'white';
  ctx.font = '20px sans-serif';
  ctx.fillText(`FPS: ${Math.round(scene.getFPS())}`, 10, 30);
  // (Pretend scene.getFPS() exists – or you could get from Profiler's data if accessible)
  ctx.fillText(`Score: ${playerScore}`, 10, 60);
}, { fps: 60 });
hudRenderer.start();

Here we assumed you have a canvas with id “hudCanvas” overlaying the main canvas (you can position it with CSS to overlap). We created a Renderer2d with a draw function that clears the HUD and draws some text (like FPS and a game score). We pass fps: 60 to throttle it to 60fps – since our main scene might be running uncapped or at high FPS, we don’t necessarily need to redraw UI more than 60fps. The delta parameter can be used if we want time-based motion in 2D as well (e.g., animating a 2D icon).

Renderer2d can also be used alone. For example, if you just want to make a canvas-based drawing app or animation, you can use it outside of any Scene/Renderer3D context. It’s essentially a convenience wrapper around requestAnimationFrame.

Using the Particle System

The particle system is a bit separate. If you want some particle effects, you can spin one up easily:

import { ParticleRenderer } from 'leaf';
const particleCanvas = document.getElementById('particleCanvas');
const particles = new ParticleRenderer(particleCanvas, { 
  particleCount: 1000,
  emissionRate: 50,
  gravity: 9.81,
  particleLifespan: 5.0 
});

This will create 1000 particles that emit at a rate of 50 per frame (or per second, depending on how it’s defined), falling down with gravity ~9.81 (earth gravity units), each particle living 5 seconds. The ParticleRenderer will immediately start the simulation on particleCanvas. If you want those particles in your 3D world, one approach is to position particleCanvas overlaying the main canvas and draw particles in a way that visually matches (not true 3D integration, but as an overlay effect like sparks or fullscreen rain). True 3D particle integration (with perspective) might come in future updates where the particle system could use the Scene’s camera matrices.

You don’t control the particle loop manually; it runs internally. If needed, you could stop it by disposing the ParticleRenderer (no explicit API given, but you might null it or remove the canvas to stop it).

Handling User Input and Interaction

Leaf.js doesn’t yet provide a high-level input system, but you can use standard DOM events on the canvas. For example:

canvas.addEventListener('click', (e) => {
  const rect = canvas.getBoundingClientRect();
  const x = e.clientX - rect.left, y = e.clientY - rect.top;
  const pickedId = renderer.pick(x, y);
  if (pickedId !== null) {
    console.log("Clicked on object with ID", pickedId);
    // We could toggle something on that object
  }
});

This pseudocode assumes the renderer might have a pick(x,y) method to utilize the pickingTexture. If not exposed, you could implement picking by reading the pixel from the pick texture via a GPU command (WebGPU has device.queue.readTexture and WebGL has gl.readPixels). For now, this is a low-level task. The Gizmo internally uses mouse events to drag axes – it attaches its own listeners via attachInteraction ￼. So if you want to allow object manipulation, you’d create a Gizmo for the selected object:

const gizmo = new Gizmo(renderer.device, modelMatrix);
gizmo.attachInteraction(canvas, camera);
gizmo.init(renderer.format);  // format = GPUTextureFormat of the canvas

Then each frame call gizmo.draw(passEncoder) in the renderer. The user can then click/drag axes to move the object. The details of applying the gizmo’s result to the object’s matrix are hidden in Gizmo’s internals; presumably it updates modelMatrix as you drag, which you would then use for the object’s transform in rendering. This part of the API is likely to change as the engine matures.

Putting it All Together

You can integrate Leaf.js in a project by using it for what it currently excels at:
	•	Rapidly visualizing 3D models in browser with minimal setup.
	•	Offloading heavy lifting (rendering, particles) to GPU for performance.
	•	Combining 2D and 3D elements (e.g., a 3D scene with a 2D UI and particle effects).

For example, imagine a simple game: you have a 3D character model in a scene, a 2D UI overlay for health and score, and some particle effects for explosions. With Leaf:
	•	Load the character model via Renderer3D.init('character.fbx').
	•	Use scene.update() to handle character movement or animations each frame.
	•	Use a <canvas> with Renderer2d for UI, updating it with game stats.
	•	Trigger ParticleRenderer when an explosion happens (rendering on another canvas or even the same with proper compositing).

It’s important to note that at this stage, Leaf.js might require you to do some manual wiring (e.g., updating object transforms or synchronizing multiple canvases). But it provides the building blocks to get these systems running efficiently.

Missing Features, Gaps, and Extending Leaf.js

Because Leaf.js is an incomplete work-in-progress, there are several features not yet fully implemented or areas where you as a developer might need to extend or work around things:
	•	Advanced Lighting and Materials: Currently, lighting is very basic (the default shader is effectively unlit or uses a simplistic lambertian model). Features like point lights, shadows, reflections, PBR materials, etc., are not in place yet. These are high on the roadmap – physically-based rendering and realistic lighting are planned ￼, but for now any complex lighting would require customizing shaders yourself. To extend, you could write a custom WGSL shader and replace the shaderModule in a parser with your code (if you’re comfortable digging into Leaf’s internals). The engine’s structure will accommodate new lighting models in the future.
	•	Physics Engine: The code mentions a PhysicsSystem in the vision, but no actual physics integration exists yet (gravity in particle system aside). Objects won’t automatically collide or fall. If your project needs physics, you’ll have to integrate an external library (like Cannon.js or Ammo.js) and then on each frame update the positions of Leaf models via scene.update callbacks. Leaf’s planned physics will tightly sync with the rendering and animation systems ￼, but until that lands, physics is entirely manual.
	•	Animations: While the theoretical API shows animations declared in the scene, the current version doesn’t provide an animation system. If you load an FBX with animations, they will not play because there’s no skeletal animation processor implemented. If you need animations, you could perhaps export your models as a series of static frames and manually swap them (inefficient), or better, incorporate a small animation engine. Since Leaf can expose the raw data, one could implement a skeletal shader that takes bone matrices uniform and modify the vertex positions. But that’s non-trivial. We anticipate that as WebGPU matures, Leaf will add GPU skinning support.
	•	Scene Graph Hierarchies: There isn’t yet support for parent-child relationships between objects (e.g., attaching a sword model to a character’s hand so it moves with it). You would have to manually update the child’s transform when the parent moves. The future “subscenes” concept ￼ suggests hierarchical scenes will be supported. As an extension, you could maintain your own tree of transforms and compose matrices before sending to Leaf (since the Camera has a view matrix, you could similarly combine parent-child matrices for models manually).
	•	Declarative API / Virtual DOM: As noted, the nice Leaf.scene({ models: {...}, animations: {...} }) syntax is not actually implemented yet. You must use the imperative classes (Scene, Renderer3D, etc.). This means no automatic diffing of scene changes – you handle adding or removing objects manually. If you remove an object, ensure to also free its GPU resources (WebGPU will free on GC if no reference, but better to explicitly destroy buffers if API allows, to avoid GPU memory leaks). Keep an eye on the library updates for when the declarative API becomes available; it will greatly simplify scene setup.
	•	WebGPU Fallback Limitations: While Leaf can fall back to WebGL2, not all features translate. Notably, the particle system using compute shaders will not work on WebGL (since WebGL has no compute). So on older browsers, particles might simply do nothing or require a different code path. Also, shaders are written in WGSL for WebGPU; Leaf would need separate GLSL shaders for WebGL. If you run in WebGL mode, the engine may use a very basic shader or possibly show an error for unsupported features. As a developer, you should test your app in a WebGL-only environment if you want broad support, and avoid the features that need WebGPU (or detect and warn the user). Over time, as WebGPU becomes standard, this will be less of an issue.
	•	Error Handling and Stability: Being a new project, you might hit unpolished edges. For example, passing a wrong file path may not produce a friendly error (it could just fail silently or throw a generic error). It’s wise to wrap renderer.init() in a try/catch or promise error handler to catch load failures. Also, large models might be slow to parse on the main thread – consider using WorkQueue to parse in a worker (Leaf doesn’t automatically do that for all formats yet, except perhaps Aseprite/PSD). If something goes wrong (e.g., nothing renders), use the browser console and the Profiler’s call stack to debug – maybe an onUpdate error occurred or the GPU shader compilation failed (WebGPU will log compilation errors in console).
	•	Extending the Engine: Leaf.js is open source, so one way to extend it is to modify the library itself. Short of that, here are some extension ideas:
	•	Custom Shaders: If you want a different visual style (say a wireframe or a toon shader), you can create your own WGSL code. You could subclass or duplicate the existing parser (e.g., make CustomOBJParser extending WebGPUOBJParser) and override getShaderString() to return your shader code, then use that to load your model. The engine will compile your shader instead. You must ensure your shader inputs (vertex attributes, uniform structure) match what the engine expects or adjust the pipeline creation accordingly.
	•	New Format Parsers: You can add support for other model formats by writing a parser similar to OBJ/FBX. For instance, glTF is a popular modern format; you could integrate a glTF parser (maybe leveraging existing JS libraries to get the data, then feed to Leaf). Since Leaf’s architecture separates parsing from rendering, you can parse the glTF into buffers and then create a Model and pipeline using Leaf’s device and pipeline creation methods.
	•	Plugin System: While the plugin API isn’t there yet, you can still modularize features by writing your own modules that use Leaf’s core. For example, you could write a Terrain Plugin that procedurally generates a terrain mesh and then uses Renderer3D to display it. Or an AI plugin that on each onUpdate moves NPCs. These aren’t “plugins” in the engine sense yet (since no official registration), but you can structure them as separate classes that operate alongside Leaf’s Scene.
	•	Contributing: If you’re comfortable, consider contributing to the project. There are only a few contributors now, and issues are open on the GitHub. For example, if you implement a cool feature like skeletal animation or a new loader, you could make a pull request. The engine is at an early stage where community contributions can shape its direction.
	•	Documentation Gaps: Aside from the README vision and code comments, documentation is sparse. This guide itself is an attempt to fill that gap. In practice, don’t hesitate to read the source code – being in TypeScript, it’s quite readable. The type definitions we cited (interfaces for Model, etc.) can guide you on what properties exist ￼. Also look at the example usages (perhaps in a public/ folder of the repo if any). Until formal docs are written, the repo and this guide are your references.

In conclusion, Leaf.js is an ambitious engine bringing together many modern web technologies (WebGPU, WASM/Workers, modern JS syntax). While incomplete, it already allows setting up a basic rendering loop with imported models and offers unique extras like a built-in Profiler and binary file parsing. Its architecture of Scenes, Renderers, and event-driven updates will be familiar to those who know game engines like Unity or Three.js – but with the twist of using WebGPU’s power and a declarative mindset for future expansion. As the WebGPU era dawns, Leaf.js positions itself as a potential all-in-one solution for web games and simulations, and integrating it early gives you a chance to influence its development and be prepared for its future capabilities ￼.

References and Sources
	•	Leaf.js Official README and Documentation ￼ ￼ ￼ ￼ ￼
	•	Leaf.js Source Code – Scene management, Event system, Renderer, etc. ￼ ￼ ￼ ￼ ￼
	•	Leaf.js Source Code – Asset Parsers and Types (OBJ, FBX, MTL, Aseprite, Particle shaders) ￼ ￼ ￼ ￼ ￼
	•	Leaf.js Source Code – Profiler and Renderer2D components ￼ ￼
	•	WebGPU and WGSL specifications (for understanding shader code and GPU device setup) ￼ ￼
	•	The Leaf.js NPM Package information (version and usage) ￼ (for confirming install details)